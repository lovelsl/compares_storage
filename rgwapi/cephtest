
没有写的情况下测试


1、实现所有rgw的成功连接
    s3账户是全体对象网关共有的，所以创建后，就是全部可以访问的，无须其它操作。



2、验证三台中任意一台挂了之后，数据能够正常获取
    挂了的结点，立刻恢复      --存储资源影响
            -- node3  reboot
            -- RAW=332G， AVAIL=868G
               POOL AVA=261G
            -- node3时间不同步，一段时间自己恢复。
            掉线瞬间，CEPH 进入 Degraded状态。
            集群显示一个结点掉线，两个OSD掉线down,。即4,up 6,in

            重启后：
                时间自动同步
                OSD自动启动，连接到集群显示up，in
                结点也显示连接
                集群自动取消Degraded状态  --> 恢复正常状态

    关闭结点，一段时间后恢复   --存储资源影响
            -- node3  poweroff
            1、关闭机器的同时，对所有数据进行访问，确认全部可以访问
                结点关闭期间
                -- RAW=332G， AVAIL=868G   SIZE = RAW + AVA
                   POOL AVA=261G

            2、等待数据重新平衡后，对所有数据进行访问，确认全部可以访问
                -- RAW=221, AVAIL=579    SIZE = RAW + AVA
                   POOL AVA=268
                存储空间部分丢失。但是数据不会丢失，全部查找到


            3、将机器重新开机，确认所有数据可以访问，并对集群资源进行测试
                结点自动识别，自动全部上线。
                -- RAW=332G， AVAIL=868G   SIZE = RAW + AVA
                   POOL AVA=261G
                -- node3时间不同步，ntpdate同步后，一段时间自己同步



3、验证三台，全部死亡后，数据能够正常获取（模拟突然断电）
   断电模拟： 先后在三台机器上poweroff
            安静等待一段时间后


        1、三台机器在差距不大的时间内启动
            -- RAW=332G， AVAIL=868G   SIZE = RAW + AVA
               POOL AVA=261G


        2、对任意一台机器，采取稍等一段时间后，重新启动
            在第三台机器未开机期间
            数据容量降低到三台机器关闭一台的状态
             -- RAW=221, AVAIL=579    SIZE = RAW + AVA
                POOL AVA=268
            期间数据查询正常

            第三台机器启动后
            一段时间后，结点自动识别，完成数据恢复
            -- RAW=332G， AVAIL=868G   SIZE = RAW + AVA
               POOL AVA=261G
            数据查询正常


4、接收到断电通知后关闭机器的操作

5、重复IP的影响
    复制node3生成node4产生的影响
        .....
        该状态下，并没有等待结点完全平衡。感觉没必要
        复制会造成ceph对OSD的识别冲突，最终MON会随机关闭OSD




6、任意机器，抽掉一个磁盘的影响
   拔掉磁盘，重新接上，对集群的影响
    无法模拟

7、扩容操作
    新OSD的连接，直接导致数据自动平衡

    扩容后测试，结点有死亡，是否会自动数据平衡


7、某一个机器彻底死亡，使用其它机器进行替换








